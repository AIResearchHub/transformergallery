{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZdGRSsEnBcujQmRxDNYFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunSohur/Self-Attention/blob/master/Self_Attention_Spelled_Out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention Spelled Out"
      ],
      "metadata": {
        "id": "z8jM-bQZahha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we will review the structure of a multi head attention module using self attention.\n",
        "\n",
        "The goal in this notebook is not to create a training mechanism that trains the multihead attention weights (that will be abother notebook).  \n",
        "\n",
        "Moreover, the goal is to be able to see how self attention - and multi head attention by extention -  look and interact in code.\n",
        "\n",
        "This version of sef attention is not the most rubust, clean, nor efficient implementation; rather, it aims to be a gentle introduction to enable understanding more complicated architecture.\n",
        "\n",
        "We recommend that you understand self attention theoretically before trying to understand this code.  If you're unfamiliar with how attention works (particually the significance of queries, keys, and values), check out our guide to self attention:\n",
        "\n",
        "https://github.com/ArjunSohur/transformergallery/blob/main/README.md\n",
        "\n",
        "The code is heavily based off of the article:\n",
        "\n",
        "https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
        "\n",
        "This article seemed to have the most consie and down-to-first-principles based self attention implementation we've seen.  Since it helped us in our journey to understanding multi head attention, we think it can greatly benefit you as well!\n",
        "\n",
        "Enjoy!"
      ],
      "metadata": {
        "id": "JBFIabIjaoyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "XR6WY4z4cNNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6qH4_P2TaHq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as f"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention Head"
      ],
      "metadata": {
        "id": "r9xyFVL-ciLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with implementing a single self attention head.  The most common formula for self attention is the scaled dot product attention: $$\\text{attention(Q, K, V)} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_{QK}}})V$$\n",
        "Where $Q, K \\in \\mathbb{R}^{n \\text{ } \\times \\text{ } d_{QK}} \\text{ and } V \\in \\mathbb{R}^{n \\text{ } \\times \\text{ } d_V } $.  If you are confused about what these $Q, K, \\text{ and } V$ come from, check out our explanation:\n",
        "https://github.com/ArjunSohur/transformergallery/blob/main/README.md\n",
        "\n",
        "Our first task will be creating this formula.  The biggest challenge - and one that we will encounter quite often - will be making sure that the dimensions of our matrices match up for matric multiplication."
      ],
      "metadata": {
        "id": "Y4QQEZYfclbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMS\n",
        "# queries - tensor of shape (batch_size, sequence_length, number_of features_for_queries)\n",
        "# keys - tensor of shape (batch_size, sequence_length, number_of features_for_keys)\n",
        "# Note: number_of features_for_keys = number_of features_for_queries\n",
        "# values - tensor of shape (batch_size, sequence_length, number_of features_for_values)\n",
        "def scaled_dot_product_attention(queries: Tensor, keys: Tensor, values: Tensor) -> Tensor:\n",
        "    # First, we batch matrix multiply the queries vector with the transpose of the keys vector\n",
        "    # This step essentially determines how important each position is relative to each other\n",
        "    # Results in a sequence_length x sequence_length matrix\n",
        "    matrix_mult_of_queries_and_keys = queries.bmm(keys.transpose(1, 2))\n",
        "\n",
        "    # We need to ensure that back propagation runs smoothly\n",
        "    # We keep the values of our above matrix multiplication in check by dividing by the square root\n",
        "    # of the number of hidden dimension of the queries (which equals the hidden dimension of the keys)\n",
        "    scalar_for_gradient_stability = queries.size(-1) ** (1/2)\n",
        "\n",
        "    # Dividing the values of the matrix multiplication by the number of hidden dimension of the queries\n",
        "    matrix_multiplication_adjusted_by_scalar = matrix_mult_of_queries_and_keys / scalar_for_gradient_stability\n",
        "\n",
        "    # To further help gradient descent and to standardize weights, we apply softmax row-wise on out result\n",
        "    softmax_scaled_matrix_multiplication = f.softmax(matrix_multiplication_adjusted_by_scalar, dim=-1)\n",
        "\n",
        "    # Lastly, we perform batch matrix multiplication with the value matrices\n",
        "    scaled_dot_product_attention_result = softmax_scaled_matrix_multiplication.bmm(values)\n",
        "\n",
        "    # Results in a sequence_length x number_of features_for_values matrix\n",
        "    return scaled_dot_product_attention_result"
      ],
      "metadata": {
        "id": "fCMJlj1ZcRBG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the formula down, we can actually focus on the self attention head itself.  We represent it as a class so that we can instantiate as many separate self attention mechanisms as possible.\n",
        "\n",
        "There are two methods in our self attention class: the instantiation method and the the forward method.\n",
        "\n",
        "The initialization method defines linear, single-layer neural networks that will act as the weigths for our ** queries, keys, and values **.  We can view a neural network as weights if we think about the weigths of the edges of a fully connected neural network between each node as an element of a matrix.  Each row corresponds to an input nodes edge weight with the output nodes.  The neural network is important for traning the weights so that the attention mechanism leans what to pay attention to.\n",
        "\n",
        "The forward method passes the query, key, and value inputs through the weighting neural networks then sends the result to the sclaed dot product attention formula."
      ],
      "metadata": {
        "id": "3XzQu9ewg4W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    # PARAMS\n",
        "    # Weight matrices are crucial for the attention model to learn what is important and what isn't\n",
        "    # All the parameters make it so that matrix multiplication of with the queries, keys, and values goes smoothly\n",
        "    # Considering the queries, keys, and values matrices will be sequence_length x embedding_length\n",
        "    # we use the embedding length as the number of rows and chose a number for the amount of hidden dimensions\n",
        "    def __init__(self, embedding_dimension: int, queries_keys_hidden_dimension: int, values_hidden_dimension: int):\n",
        "        # Since SelfAttentionHead is a subclass of nn.module, we need to make a super call\n",
        "        super(SelfAttentionHead, self).__init__()\n",
        "\n",
        "        # Weights for thr keys, queries, and values\n",
        "        self.query_weights = nn.Linear(embedding_dimension, queries_keys_hidden_dimension)\n",
        "        self.key_weights = nn.Linear(embedding_dimension, queries_keys_hidden_dimension)\n",
        "        self.value_weights = nn.Linear(embedding_dimension, values_hidden_dimension)\n",
        "\n",
        "    # Overriding nn.Module's forward method\n",
        "    # PARAMS\n",
        "    # query, key, and values are all have size input_sequence_length x embedding size\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        # performing matrix multiplication with the weights\n",
        "        weighted_query = self.query_weights(query)\n",
        "        weighted_key = self.key_weights(key)\n",
        "        weighted_value = self.value_weights(value)\n",
        "\n",
        "        # using scaled dot product attention to find the attention weights\n",
        "        attention = scaled_dot_product_attention(weighted_query, weighted_key, weighted_value)\n",
        "\n",
        "        return attention"
      ],
      "metadata": {
        "id": "gwJLp952n3us"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention"
      ],
      "metadata": {
        "id": "vTaxiy2f5-eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we called the above class \"SelfAttentionHead\".  Then, the name \"multi head attention\" would imply that there are multiple self attention heads - which is exactly the case!\n",
        "\n",
        "Our multi head attention class is just multiple \"heads\" of self attention.\n",
        "\n",
        "We send our inputs through multiple heads of attention and concatenate the results of each one.  To normalize the jumble of all the concatenated attention head outputs, we send it though a simple one layer neural network that outputs a normal sized input vector that encapsulates the outputs of the attention head.\n"
      ],
      "metadata": {
        "id": "XUICXPx65xZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # Same params as a regular self attention head, but we have to specify how many heads we want\n",
        "    def __init__(self, number_of_heads: int, embedding_dimension: int, queries_keys_hidden_dimension: int,\n",
        "                 values_hidden_dimension: int):\n",
        "        # Since MultiHeadAttention is a subclass of nn.module, we perform a super call to begin with\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Creates a list of heads\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(embedding_dimension, queries_keys_hidden_dimension,\n",
        "                                                      values_hidden_dimension)\n",
        "                                    for _ in range(number_of_heads)])\n",
        "\n",
        "        # feed forward layer to deal with the huge concatenation matrix\n",
        "        self.feed_forward_layer = nn.Linear(number_of_heads * values_hidden_dimension, embedding_dimension)\n",
        "\n",
        "    # forward call\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor):\n",
        "        # We basically just concatenate all the results of each head ...\n",
        "        multi_head_result = torch.cat([head(query, key, value) for head in self.heads], dim=-1)\n",
        "\n",
        "        # ... then pass it through a feed forward neural network to clean it up\n",
        "        processed_multi_head_result = self.feed_forward_layer(multi_head_result)\n",
        "\n",
        "        return processed_multi_head_result"
      ],
      "metadata": {
        "id": "ydFS5KJD5t0S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running an Input Through a Multi Head Attention Layer"
      ],
      "metadata": {
        "id": "-7Q8FqsI8mPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, let's test out this bad Larrold out to see if it gives us the correct size tensor based on the input."
      ],
      "metadata": {
        "id": "rCMIjTfA8w8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Here I just made up numbers to see if the multi-head attention mechanism woks as intended\n",
        "    # The goal of the code is not to use or train multi-head attention, but just to create it\n",
        "    # therefore, the determinant of success will be if the output is valid\n",
        "    number_of_batches = 3\n",
        "    number_of_inputs = 24\n",
        "    embedding_dimension = 512\n",
        "    queries_and_keys_hidden_dimension = 1024\n",
        "    values_hidden_dimension = 512\n",
        "    number_of_heads = 8\n",
        "\n",
        "    # Creating random values for keys, queries, and values\n",
        "    query = torch.rand([number_of_batches, number_of_inputs, embedding_dimension])\n",
        "    key = torch.rand([number_of_batches, number_of_inputs, embedding_dimension])\n",
        "    value = torch.rand([number_of_batches, number_of_inputs, embedding_dimension])\n",
        "\n",
        "    # Initializing a multi-head attention instance\n",
        "    multi_head = MultiHeadAttention(number_of_heads=number_of_heads,\n",
        "                                    queries_keys_hidden_dimension=queries_and_keys_hidden_dimension,\n",
        "                                    embedding_dimension=embedding_dimension,\n",
        "                                    values_hidden_dimension=values_hidden_dimension)\n",
        "\n",
        "    # Seeing the result of a forward pass\n",
        "    print(multi_head.forward(query, key, value).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j95LZKU8vem",
        "outputId": "70ef1ce9-6563-4591-ae2c-a016033c50d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 24, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And it looks like, though all that jumble, we got a vector that is the same size as our input, which is exactly what we wanted!\n",
        "\n",
        "If we had trained this attention layer, our output would now have important relationships, like dependancies, between different parts of the input encoded into it."
      ],
      "metadata": {
        "id": "mANUMHSP9VxG"
      }
    }
  ]
}